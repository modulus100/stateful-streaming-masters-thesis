%Some basic ways to manipulate text are \textit{italics} and \textbf{bold}. One can reference Figures (see Figure \ref{fig:taltech} for an example) as well as cite references which are defined in the \textit{references.bib} file.\cite{spectre,example-reference}
%
%The \textit{Bibliography}, \textit{List of Figures} and \textit{List of Tables} are all automatically generated and references will be updated automatically as well. This means that if you've defined a citation but are not referencing it, it will not appear in the \textit{Bibliography}. This also means that any Figure / Table / Citations numbers are automatically updated as well. Numbering is done by order-of-appearance.
%
%One can create an itemized list:
%\begin{itemize}
%    \item item a
%    \item item b
%    \item ...
%\end{itemize}
%
%Or enumerate them:
%\begin{enumerate}
%    \item item x
%    \item item y
%    \item ...
%\end{enumerate}
%
%
%\begin{figure}[ht]
%    \centering
%    \includegraphics[width=.5\textwidth]{figures/taltech.jpg}
%    \caption{\textit{An image of the TalTech logo.}}
%    \label{fig:taltech}
%\end{figure}
%
%
%A table with three columns can be seen in Table \ref{tab:requirements}.
%\begin{longtable}{|p{0,5cm}|p{10cm}|p{3cm}|}
%	\caption{\it{A table with some requirements}}
%	\label{tab:requirements}\\ \hline
%	\textbf{Nr} &  \textbf{Requirement} & \textbf{Weight}  \\
%	\hline
%	\endfirsthead
%	\multicolumn{3}{l}%
%	{\tablename\ \thetable\ -- \textit{Continues...}} \\ q
%	\hline
%	\textbf{Nr} &  \textbf{Requirement} & \textbf{Importance}  \\
%	\hline
%	\endhead
%	\hline \multicolumn{3}{l}{\textit{Continues...}} \\
%	\endfoot
%	\hline
%	\endlastfoot
%1 & Price & High\\ \hline
%2 & Variety& Middle\\ \hline
%3 & Support& Low\\ \hline
%
%\end{longtable}
%
%We can use variables set in the \textit{main.tex} file to render values like our title (\doctitle) or supervisor names (\textbf{Supervisor}: \supervisor, \textbf{Co-supervisor}: \cosupervisor{}).
%\section{What is a stream processing}\label{sec:stream-beg}
%Stream processing
%
%\section{Brief history of stream processing systems}\label{sec:breif-history}
%Stream processing


\section{Background and motivation}\label{sec:back-and-motiv}
Software engineering is a diverse field that addresses various business domain problems.
These domain-specific issues and their corresponding use cases necessitate in-depth analysis to determine the most suitable technologies for achieving optimal problem-solving outcomes.
One rapidly growing area in software engineering is big data.
Although big data is sometimes regarded as a marketing term, it encompasses complex data processing frameworks and datasets.

As the volume of produced data has dramatically increased over the years, modern technical solutions capable of processing massive amounts of data are required.
The open-source community offers decent frameworks and tools, making it challenging to choose the best option.
This research focuses on comparing the most suitable frameworks for specific use cases, particularly those that are designed for stateful real-time stream processing.

Stream processing use cases are relatively rare compared to typical problems that can often
be solved with traditional batch processing or simple programs that don't require the MapReduce model.
However, stateful stream processing assumes that real-time processing relies on previous states and unbounded data flow, which may indicate abnormal system behavior, such as fraud alerts in financial transactions
within a specific time frame.
Quick response can facilitate necessary actions and save time.
Therefore, it is crucial to provide a technical analysis of the most suitable frameworks, assess their advantages and disadvantages, and examine the complexities involved in their application for stateful stream processing based on a given use case.
This master's thesis offers a technical overview, benchmarks, and comparisons for appropriate use cases.
The use case considers that incoming events trigger state aggregation and re-computation and evaluates how efficiently Kafka Streams \cite{kafka_streams_intro} and Apache Flink \cite{flink_intro} manage the state under heavy loads, represented by a stream of Kafka \cite{kafka_intro} messages.
The ideal framework should address existing problems while offering excellent scalability, state recovery, and cost efficiency.

\newpage
\section{Problem statement}\label{sec:prob-st}
The problem comes with business demands that are based on current needs for a more scalable and efficient solution.
Before getting into the problem details, the current model will be provided to better understand why this research
brings value and how streaming frameworks are comparable with the batch processing model \ref{fig:current-model}.

\subsection{Batch processing model overview}\label{subsec:current-model}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{figures/current-model-2}
    \caption{\textit{Generic model of a data processing model which should be improved.}}
    \label{fig:current-model}
\end{figure}

The Figure \ref{fig:current-model} represents only a very generic model of the current
system to just get some overview.
The model contain the following components.

\begin{description}
    \item[Kubernetes] manages life cycle of components like database and job \cite{kubernetes_intro}.
    \item[Data course N] is an external component which sends a json messages in unbound mode.
    Each data source is located in different external network and gets connected through
    a network gateway before it reaches the job.
    \item[Perdiodic batch processing job] represents a functionality block which uses queries to access the
    data lake and save intermediate state to the state database.
    The job represents an application written in highly-level object-oriented programming language
    and works in a batch processing mode which means it gets launched periodically to process a batch of data records.
    \item[State storage] contains a state for each executed query.
    Contains an information from previous query executions.
    \item[Database] is a storage with data records from all data sources.
\end{description}

From a first look, the system which is on the Figure \ref{fig:current-model} defines a straightforward
architecture.
Generally, the systems which implements such architecture are easily debuggable,
testable and quite often is a good way to go when the project gets started.
Everything might work smoothly for ages.
At some moment in time, the system might face with a breaking point which brings new challenges.

For example, in a few years, the number of data sources has significantly grown, where each
data source has its own sending frequency and a type.
A new load would require to add more and more instances of a job, faster period of job execution.
And also it leads to additional types of errors to handle, which haven't been seen before.
An execution of each query gets more expensive.
Not every data can be indexed, and especially if amount of indexed data has grown in many times, it adds
a significant complexity overhead for a query execution.
Moreover, additional errors might and circumstances may get the service down, which requires
additional balancing logic and a state recovery.
At might take a significant time to recover a service in specific region if this requires a manual work
from engineering team.

As a summary, some crucial problems can be defined as follows.

\begin{itemize}
    \item The system is not scalable enough.
    \item A state is lost after recovery, not completely fault tolerant.
    \item Expensive query execution, especially if data is not indexed
    \item Job processing is periodic, leads to a latency and limitation in processing time.
\end{itemize}

\newpage
\section{Research question and objectives}\label{sec:res-q-o}
The main question is to figure out how different frameworks behave in case of fault tolerance,
for example, if some processing replicas or workers are down.
How would load rebalancing work, how much delay would it create,
and how long would it take for the system to restore a state?
The research consists of several tasks.

\begin{itemize}
    \item Create prototypes with Apache Flink and Kafka Streams.
    \item Prepare cloud setup in order to make experiments repeatable.
    \item Define metrics and benchmarks.
    \item Execute experiments.
    \item Collect results for analysis.
    \item Plot the data and make a conclusion.
\end{itemize}


%\subsection{Alternative architectures}\label{subsec:alternative-architectures-and-frameworks}
%Fortunately, similar problems have already known in IT industry and big players like Google, Microsoft
%have invested enough resources.
%Their research results lead to a stream processing.
%Stream processing is tend to be as alternative for bach processing which works for most cases, but
%this specific use case if focused on real time data.
%Once it's clear that stream processing is the way to go then comes a next question.
%Another solution is to come up with a custom solution.
%However, the most significant disadvantages include:
%
%\begin{itemize}
%    \item A need to hire a team just to developer another stream processing framework.
%    \item There's a big change that a custom solution won't perform better than existing.
%    \item Long-term engineering support, such as bug fixes, integration with external dependencies,
%    feature development.
%    \item The cost can become prohibitive.
%\end{itemize}
%
%
%At the same time, the open source community is huge enough, which supports the greatest
%open source products under The Apache Software Foundation.
%The most significant disadvantages include:
%
%\begin{itemize}
%    \item Industry standard frameworks.
%    \item A huge support from developer around the world.
%    \item An ease in extending already made code.
%    \item Lots of ready for use tutorials and documentations.
%    \item Many examples for different use cases.
%\end{itemize}


%\newpage
\subsection{Selecting a suitable framework}\label{subsec:selecting-a-suitable-framework}
They're lots different frameworks out there available for solving different
data streaming problems.
Most of the were born on top of each other as a new generation solution.
Each new framework is trying to bring better performance and scalability.
Down below is an evolution of streaming frameworks \cite{stream_processing_history} with a brief description:

\begin{description}
    \item[Apache S4]  Apache S4 (Simple Scalable Streaming System) was an early stream
    processing engine developed at Yahoo! Labs.
    It was designed for unbounded data stream processing which uses
    simple programming model based on a publish/subscribe pattern.
    Apache S4 became an open source project under Apache Software Foundation but
    eventually became inactive due to limited community adoption \cite{apache_s4_intro}.
    \item[Apache Storm]
    Apache Storm was a popular distributed stream processing framework.
    It was designed for real-time data processing and provided guarantees such as
    at-least-once and exactly-once processing semantics.
    Apache Storm was more popular comparing to Apache S4, but it provides less
    performance and flexibility comparing to next generation frameworks \cite{apache_storm_performance}.
    \item[Apache Flink] Originally was developed at the Technical University of Berlin.
    Apache Flink is one of the most powerful stream processing framework that unified batch and
    stream processing with focusing on streaming.
    Flink provided low-latency, high-throughput, and exactly-once processing semantics.
    With its advanced features, such as event time processing, watermarks, and savepoints,
    Flink has become the one of the most popular stream processing framework which is
    well-designed for highly loaded complex data stream processing use cases \cite{flink_intro}.
    \item[Kafka Streams] Introduced as part of Apache Kafka, Kafka Streams
    is a rather a stream processing library than a framework which allows developers to build real-time
    applications and microservices using the Kafka platform.
    Kafka Streams provides a simple, functional programming model and is tightly
    integrated with the Kafka ecosystem.
    It's well-suited for use cases like real-time analytics, data transformation
    and event-driven architectures.
    Kafka Stream can a replacement for Apache Flink for cases where heavy integration
    and complex computation is not needed \cite{kafka_streams_intro}.
    \item[Apache Spark] Is well known alternative for Apache Flink and Kafka Streams,
    is a popular solution for most of the cases, but does a micro-batching \cite{spark_structured_streaming}.
\end{description}

However, for this study Apache Flink and Kafka streams were chosen as most popular real-time processing
framework which do not use micro-batching.
%Also, both have a great open source community support.
%However, some companies fork these projects to add additional features for their use cases
%\cite{stream_processing_frameworks_overview}, \cite{flink2019}, \cite{kafka2020},
%\cite{kleppmann2017}, \cite{kleppmann2016making}.

\subsection{Deployment environment}\label{subsec:deployment-environment}
In 2024, the most popular and advanced application container manager is Kubernetes.
First, stream processing frameworks were designed to be run on YARN clusters.
YARN is no longer considered to be the preferred deployment manager.
It means that a framework must be able to run in Kubernetes using containers.
All experiments for this study were conducted with an AWS cloud provider.


\subsection{Requirements summary}\label{subsec:final-requirements}
At the moment, there are two leading frameworks are available which
should help to build efficient and reliable solution, and
they are Apache Flink and Kafka Streams.
Both are designed for data streaming problems, have a large community support.
More detailed comparison will are described in the next section.

As a summary, these important bullet points provided down below which will
be considered during the evaluation for the new solution.

\begin{description}
    \item[Scalability] The current is not scalable enough.
    \item[Fault tolerance] A state must not be lost if a pod where job is running got down,
    the system should be able to proceed processing with previous state once job is recovered.
    \item[Simple integration] A solution, weather it is a programming language,
    or framework, should be compatible with the current technology stack, such as JVM and Kubernetes.
    It also means that a solution won't require hiring an entire engineering department or a team.
    \item[Cost effecienty] It's quite important that a highly loaded solution doesn't cost too much.
    \item[Cloud independence] A solution must not depend on a cloud provider.
    \item[Functionality] A solution has to have an API which allows to implement and test a considered use case.
\end{description}


